{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ef3f7-806b-4ba2-97aa-a89bc85cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251480ff-368c-4272-8a8e-e0b5d6890b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macwa\\AppData\\Local\\Temp\\ipykernel_32220\\1852528703.py:12: DtypeWarning: Columns (33,36,37,39,41,42,43,44,45,46,47,48,49,50,51,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(local_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file:\n",
      "['CRASH DATE', 'CRASH TIME', 'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED', 'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5', 'COLLISION_ID', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5', 'UNIQUE_ID', 'VEH_COLLISION_ID', 'CRASH_DATE', 'CRASH_TIME', 'VEHICLE_ID', 'STATE_REGISTRATION', 'VEHICLE_TYPE', 'VEHICLE_MAKE', 'VEHICLE_MODEL', 'VEHICLE_YEAR', 'TRAVEL_DIRECTION', 'VEHICLE_OCCUPANTS', 'DRIVER_SEX', 'DRIVER_LICENSE_STATUS', 'DRIVER_LICENSE_JURISDICTION', 'PRE_CRASH', 'POINT_OF_IMPACT', 'VEHICLE_DAMAGE', 'VEHICLE_DAMAGE_1', 'VEHICLE_DAMAGE_2', 'VEHICLE_DAMAGE_3', 'PUBLIC_PROPERTY_DAMAGE', 'PUBLIC_PROPERTY_DAMAGE_TYPE', 'CONTRIBUTING_FACTOR_1', 'CONTRIBUTING_FACTOR_2']\n",
      "Error uploading file to Azure Blob Storage: The specified blob already exists.\n",
      "RequestId:356d5310-301e-0029-7fd6-d31449000000\n",
      "Time:2024-07-11T21:05:28.6916220Z\n",
      "ErrorCode:BlobAlreadyExists\n",
      "Content: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobAlreadyExists</Code><Message>The specified blob already exists.\n",
      "RequestId:356d5310-301e-0029-7fd6-d31449000000\n",
      "Time:2024-07-11T21:05:28.6916220Z</Message></Error>\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Azure Blob Storage connection string\n",
    "azure_blob_connection_string = \"cNvsvKyR7q7rnqcpX1nU32d65m71zwW4AQ6UM5jmnDem/20inTNMBW0LnGan3xV4xTqaUiNfD2Olgr8EkJ12QMmOcFh3zU2UsI6Ylocjzd2xI4+RDGr8g6WlsfviHWD+5M70kE9pC0Gn0TMt4gWhn9pWBrL+TVgXr3HdfAeikCQW2FMuFPtBWphBWgiqp6VQsW9HvObAgYrEN8x+4tFlxc1vD2JsCHn74eqhhh05vzC7Inodp6+R6uzlOqAuJV2B\"\n",
    "\n",
    "# Local file path to upload (use raw string or escape backslashes)\n",
    "local_file_path = r\"D:\\Downloads 2\\George Brown\\Sem 2.5\\Big data 2\\Big Data project\\Files to Submit\\Transformations\\Hadoop_MVCdataNYPD\\hadoop_joined_crash_vehicles.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(local_file_path)\n",
    "\n",
    "# Get the column names from the DataFrame\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Print the column names\n",
    "print(\"Column names in the CSV file:\")\n",
    "print(column_names)\n",
    "\n",
    "# Blob Storage container name and blob name\n",
    "container_name = \"hadoopcrashvehicles\"  # Replace with your container name\n",
    "blob_name = \"hadoop_joined_crash_vehicles.csv\"  # Replace with the name you want to give to your blob/file\n",
    "\n",
    "try:\n",
    "    # Initialize BlobServiceClient using connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(azure_blob_connection_string)\n",
    "\n",
    "    # Create a blob client using the container name and blob name\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "    # Upload a file to Azure Blob Storage\n",
    "    with open(local_file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)\n",
    "\n",
    "    print(f\"File '{blob_name}' uploaded to Azure Blob Storage successfully.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    print(f\"Error uploading file to Azure Blob Storage: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44294fa-6bb9-4995-af65-551969a92345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94a35a-9359-4235-a290-15303d431fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully!\n",
      "Microsoft SQL Server\n",
      "12.00.5564\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Azure SQL Database connection details\n",
    "server = 'blahblah.database.windows.net'\n",
    "database = 'Hadoop_CrashVehiclesDataNYPD'\n",
    "username = 'azureuser'\n",
    "password = 'Ilove85workWonder69'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Create the connection string\n",
    "conn_str = (\n",
    "    f'DRIVER={driver};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'UID={username};'\n",
    "    f'PWD={password};'\n",
    "    f'Encrypt=yes;'\n",
    "    f'TrustServerCertificate=no;'\n",
    "    f'Connection Timeout=30;'\n",
    ")\n",
    "# Adjust the data types according to your actual data\n",
    "columns_with_types = \", \".join([f\"[{col}] VARCHAR(255)\" for col in column_names])\n",
    "\n",
    "# Define the CREATE TABLE statement\n",
    "table_name = \"hadoop_joined_crash_vehicles\"\n",
    "create_table_sql = f\"CREATE TABLE {table_name} ({columns_with_types});\"\n",
    "\n",
    "try:\n",
    "    # Establish a connection to the Azure SQL Database\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    \n",
    "    # Print connection details\n",
    "    print(\"Connection established successfully!\")\n",
    "    print(connection.getinfo(pyodbc.SQL_DBMS_NAME))\n",
    "    print(connection.getinfo(pyodbc.SQL_DBMS_VER))\n",
    "\n",
    "    # Execute the CREATE TABLE statement\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(create_table_sql)\n",
    "    connection.commit()\n",
    "\n",
    "except pyodbc.Error as ex:\n",
    "    print(\"Error connecting to Azure SQL Database:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761accf-a467-4ff7-aae3-458d2749131b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea97a6d-c3c8-4be9-be5a-14c549442bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macwa\\AppData\\Local\\Temp\\ipykernel_32220\\2749139235.py:23: DtypeWarning: Columns (33,36,37,39,41,42,43,44,45,46,47,48,49,50,51,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(local_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4219296 entries, 0 to 4219295\n",
      "Data columns (total 54 columns):\n",
      " #   Column                         Dtype \n",
      "---  ------                         ----- \n",
      " 0   CRASH DATE                     object\n",
      " 1   CRASH TIME                     object\n",
      " 2   BOROUGH                        object\n",
      " 3   ZIP CODE                       object\n",
      " 4   LATITUDE                       object\n",
      " 5   LONGITUDE                      object\n",
      " 6   LOCATION                       object\n",
      " 7   ON STREET NAME                 object\n",
      " 8   CROSS STREET NAME              object\n",
      " 9   OFF STREET NAME                object\n",
      " 10  NUMBER OF PERSONS INJURED      object\n",
      " 11  NUMBER OF PERSONS KILLED       object\n",
      " 12  NUMBER OF PEDESTRIANS INJURED  object\n",
      " 13  NUMBER OF PEDESTRIANS KILLED   object\n",
      " 14  NUMBER OF CYCLIST INJURED      object\n",
      " 15  NUMBER OF CYCLIST KILLED       object\n",
      " 16  NUMBER OF MOTORIST INJURED     object\n",
      " 17  NUMBER OF MOTORIST KILLED      object\n",
      " 18  CONTRIBUTING FACTOR VEHICLE 1  object\n",
      " 19  CONTRIBUTING FACTOR VEHICLE 2  object\n",
      " 20  CONTRIBUTING FACTOR VEHICLE 3  object\n",
      " 21  CONTRIBUTING FACTOR VEHICLE 4  object\n",
      " 22  CONTRIBUTING FACTOR VEHICLE 5  object\n",
      " 23  COLLISION_ID                   object\n",
      " 24  VEHICLE TYPE CODE 1            object\n",
      " 25  VEHICLE TYPE CODE 2            object\n",
      " 26  VEHICLE TYPE CODE 3            object\n",
      " 27  VEHICLE TYPE CODE 4            object\n",
      " 28  VEHICLE TYPE CODE 5            object\n",
      " 29  UNIQUE_ID                      object\n",
      " 30  VEH_COLLISION_ID               object\n",
      " 31  CRASH_DATE                     object\n",
      " 32  CRASH_TIME                     object\n",
      " 33  VEHICLE_ID                     object\n",
      " 34  STATE_REGISTRATION             object\n",
      " 35  VEHICLE_TYPE                   object\n",
      " 36  VEHICLE_MAKE                   object\n",
      " 37  VEHICLE_MODEL                  object\n",
      " 38  VEHICLE_YEAR                   object\n",
      " 39  TRAVEL_DIRECTION               object\n",
      " 40  VEHICLE_OCCUPANTS              object\n",
      " 41  DRIVER_SEX                     object\n",
      " 42  DRIVER_LICENSE_STATUS          object\n",
      " 43  DRIVER_LICENSE_JURISDICTION    object\n",
      " 44  PRE_CRASH                      object\n",
      " 45  POINT_OF_IMPACT                object\n",
      " 46  VEHICLE_DAMAGE                 object\n",
      " 47  VEHICLE_DAMAGE_1               object\n",
      " 48  VEHICLE_DAMAGE_2               object\n",
      " 49  VEHICLE_DAMAGE_3               object\n",
      " 50  PUBLIC_PROPERTY_DAMAGE         object\n",
      " 51  PUBLIC_PROPERTY_DAMAGE_TYPE    object\n",
      " 52  CONTRIBUTING_FACTOR_1          object\n",
      " 53  CONTRIBUTING_FACTOR_2          object\n",
      "dtypes: object(54)\n",
      "memory usage: 1.7+ GB\n"
     ]
    }
   ],
   "source": [
    "def log_message(message):\n",
    "    \"\"\"Function to log general messages\"\"\"\n",
    "    with open('process_log.txt', 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def log_error(error_message):\n",
    "    \"\"\"Function to log errors\"\"\"\n",
    "    with open('error_log.txt', 'a') as f:\n",
    "        f.write(error_message + '\\n')\n",
    "\n",
    "try:\n",
    "    log_message(\"Attempting to establish connection to Azure SQL Database...\")\n",
    "    # Establish a connection to the Azure SQL Database\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    log_message(\"Connection established successfully!\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    log_message(\"Reading CSV file into pandas DataFrame...\")\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    local_file_path = r\"D:\\Downloads 2\\George Brown\\Sem 2.5\\Big data 2\\Big Data project\\Files to Submit\\Transformations\\Hadoop_MVCdataNYPD\\hadoop_joined_crash_vehicles.csv\"\n",
    "    df = pd.read_csv(local_file_path)\n",
    "    log_message(\"CSV file read successfully!\")\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    df = df.astype(str)\n",
    "    log_message(\"Converted DataFrame columns to strings.\")\n",
    "\n",
    "    # Print the DataFrame structure\n",
    "    log_message(f\"DataFrame structure: \\n{df.info()}\")\n",
    "\n",
    "    # Use the entire DataFrame for insertion\n",
    "    df_subset = df\n",
    "    log_message(f\"Using all rows of the DataFrame.\")\n",
    "\n",
    "    # Print the first few rows\n",
    "    log_message(f\"First few rows of the DataFrame subset:\\n{df_subset.head()}\")\n",
    "\n",
    "    # Insert DataFrame records one by one, commit in batches\n",
    "    batch_size = 1000  # Adjust the batch size as needed\n",
    "    batch_values = []\n",
    "\n",
    "    for index, row in df_subset.iterrows():\n",
    "        values = tuple(row)\n",
    "        batch_values.append(values)\n",
    "\n",
    "        if (index + 1) % batch_size == 0 or (index + 1) == len(df_subset):\n",
    "            try:\n",
    "                insert_sql = f\"INSERT INTO hadoop_joined_crash_vehicles VALUES ({', '.join('?' * len(values))})\"\n",
    "                cursor.executemany(insert_sql, batch_values)\n",
    "                connection.commit()\n",
    "                log_message(f\"Committed {index + 1} rows successfully.\")\n",
    "                batch_values.clear()\n",
    "            except pyodbc.Error as e:\n",
    "                error_message = f\"Error inserting batch ending at row {index + 1}: {str(e)}\"\n",
    "                log_error(error_message)\n",
    "                log_message(error_message)\n",
    "                connection.rollback()  # Rollback the batch on error\n",
    "\n",
    "    # Final commit for any remaining rows\n",
    "    if batch_values:\n",
    "        try:\n",
    "            cursor.executemany(insert_sql, batch_values)\n",
    "            connection.commit()\n",
    "            log_message(f\"Committed final batch successfully.\")\n",
    "        except pyodbc.Error as e:\n",
    "            error_message = f\"Error inserting final batch: {str(e)}\"\n",
    "            log_error(error_message)\n",
    "            log_message(error_message)\n",
    "            connection.rollback()  # Rollback the final batch on error\n",
    "\n",
    "    log_message(\"Data inserted successfully into the 'call_data_lapd' table.\")\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    log_message(\"Connection closed successfully.\")\n",
    "\n",
    "except pyodbc.Error as ex:\n",
    "    # Log the connection error to the file\n",
    "    error_message = f\"Error connecting to Azure SQL Database: {str(ex)}\"\n",
    "    log_error(error_message)\n",
    "    log_message(error_message)\n",
    "\n",
    "except Exception as ex:\n",
    "    # Log any other exceptions\n",
    "    error_message = f\"General error: {str(ex)}\"\n",
    "    log_error(error_message)\n",
    "    log_message(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d264df9-5b8c-4a14-9e89-259b39f98a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
