{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ef3f7-806b-4ba2-97aa-a89bc85cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251480ff-368c-4272-8a8e-e0b5d6890b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in the CSV file:\n",
      "['date_of_interest', 'CASE_COUNT', 'PROBABLE_CASE_COUNT', 'HOSPITALIZED_COUNT', 'DEATH_COUNT', 'CASE_COUNT_7DAY_AVG', 'ALL_CASE_COUNT_7DAY_AVG', 'HOSP_COUNT_7DAY_AVG', 'DEATH_COUNT_7DAY_AVG', 'BX_CASE_COUNT', 'BX_PROBABLE_CASE_COUNT', 'BX_HOSPITALIZED_COUNT', 'BX_DEATH_COUNT', 'BX_CASE_COUNT_7DAY_AVG', 'BX_PROBABLE_CASE_COUNT_7DAY_AVG', 'BX_ALL_CASE_COUNT_7DAY_AVG', 'BX_HOSPITALIZED_COUNT_7DAY_AVG', 'BX_DEATH_COUNT_7DAY_AVG', 'BK_CASE_COUNT', 'BK_PROBABLE_CASE_COUNT', 'BK_HOSPITALIZED_COUNT', 'BK_DEATH_COUNT', 'BK_CASE_COUNT_7DAY_AVG', 'BK_PROBABLE_CASE_COUNT_7DAY_AVG', 'BK_ALL_CASE_COUNT_7DAY_AVG', 'BK_HOSPITALIZED_COUNT_7DAY_AVG', 'BK_DEATH_COUNT_7DAY_AVG', 'MN_CASE_COUNT', 'MN_PROBABLE_CASE_COUNT', 'MN_HOSPITALIZED_COUNT', 'MN_DEATH_COUNT', 'MN_CASE_COUNT_7DAY_AVG', 'MN_PROBABLE_CASE_COUNT_7DAY_AVG', 'MN_ALL_CASE_COUNT_7DAY_AVG', 'MN_HOSPITALIZED_COUNT_7DAY_AVG', 'MN_DEATH_COUNT_7DAY_AVG', 'QN_CASE_COUNT', 'QN_PROBABLE_CASE_COUNT', 'QN_HOSPITALIZED_COUNT', 'QN_DEATH_COUNT', 'QN_CASE_COUNT_7DAY_AVG', 'QN_PROBABLE_CASE_COUNT_7DAY_AVG', 'QN_ALL_CASE_COUNT_7DAY_AVG', 'QN_HOSPITALIZED_COUNT_7DAY_AVG', 'QN_DEATH_COUNT_7DAY_AVG', 'SI_CASE_COUNT', 'SI_PROBABLE_CASE_COUNT', 'SI_HOSPITALIZED_COUNT', 'SI_DEATH_COUNT', 'SI_PROBABLE_CASE_COUNT_7DAY_AVG', 'SI_CASE_COUNT_7DAY_AVG', 'SI_ALL_CASE_COUNT_7DAY_AVG', 'SI_HOSPITALIZED_COUNT_7DAY_AVG', 'SI_DEATH_COUNT_7DAY_AVG', 'INCOMPLETE']\n",
      "File 'COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv' uploaded to Azure Blob Storage successfully.\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Azure Blob Storage connection string\n",
    "azure_blob_connection_string = \"cNvsvKyR7q7rnqcpX1nU32d65m71zwW4AQ6UM5jmnDem/20inTNMBW0LnGan3xV4xTqaUiNfD2Olgr8EkJ12QMmOcFh3zU2UsI6Ylocjzd2xI4+RDGr8g6WlsfviHWD+5M70kE9pC0Gn0TMt4gWhn9pWBrL+TVgXr3HdfAeikCQW2FMuFPtBWphBWgiqp6VQsW9HvObAgYrEN8x+4tFlxc1vD2JsCHn74eqhhh05vzC7Inodp6+R6uzlOqAuJV2B\"\n",
    "\n",
    "# Local file path to upload (use raw string or escape backslashes)\n",
    "local_file_path = r\"D:\\Downloads 2\\George Brown\\Sem 2.5\\Big data 2\\Big Data project\\Covid 19\\COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(local_file_path)\n",
    "\n",
    "# Get the column names from the DataFrame\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Print the column names\n",
    "print(\"Column names in the CSV file:\")\n",
    "print(column_names)\n",
    "\n",
    "# Blob Storage container name and blob name\n",
    "container_name = \"covid19datanypd\"  # Replace with your container name\n",
    "blob_name = \"COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\"  # Replace with the name you want to give to your blob/file\n",
    "\n",
    "try:\n",
    "    # Initialize BlobServiceClient using connection string\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(azure_blob_connection_string)\n",
    "\n",
    "    # Create a blob client using the container name and blob name\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "    # Upload a file to Azure Blob Storage\n",
    "    with open(local_file_path, \"rb\") as data:\n",
    "        blob_client.upload_blob(data)\n",
    "\n",
    "    print(f\"File '{blob_name}' uploaded to Azure Blob Storage successfully.\")\n",
    "\n",
    "except Exception as ex:\n",
    "    print(f\"Error uploading file to Azure Blob Storage: {ex}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44294fa-6bb9-4995-af65-551969a92345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94a35a-9359-4235-a290-15303d431fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully!\n",
      "Microsoft SQL Server\n",
      "12.00.5564\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Azure SQL Database connection details\n",
    "server = 'blahblah.database.windows.net'\n",
    "database = 'Covid19_dataNYPD'\n",
    "username = 'azureuser'\n",
    "password = 'Ilove85workWonder69'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Create the connection string\n",
    "conn_str = (\n",
    "    f'DRIVER={driver};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'UID={username};'\n",
    "    f'PWD={password};'\n",
    "    f'Encrypt=yes;'\n",
    "    f'TrustServerCertificate=no;'\n",
    "    f'Connection Timeout=30;'\n",
    ")\n",
    "# Adjust the data types according to your actual data\n",
    "columns_with_types = \", \".join([f\"[{col}] VARCHAR(255)\" for col in column_names])\n",
    "\n",
    "# Define the CREATE TABLE statement\n",
    "table_name = \"Covid19_dataNYPD\"\n",
    "create_table_sql = f\"CREATE TABLE {table_name} ({columns_with_types});\"\n",
    "\n",
    "try:\n",
    "    # Establish a connection to the Azure SQL Database\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    \n",
    "    # Print connection details\n",
    "    print(\"Connection established successfully!\")\n",
    "    print(connection.getinfo(pyodbc.SQL_DBMS_NAME))\n",
    "    print(connection.getinfo(pyodbc.SQL_DBMS_VER))\n",
    "\n",
    "    # Execute the CREATE TABLE statement\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(create_table_sql)\n",
    "    connection.commit()\n",
    "\n",
    "except pyodbc.Error as ex:\n",
    "    print(\"Error connecting to Azure SQL Database:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b761accf-a467-4ff7-aae3-458d2749131b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea97a6d-c3c8-4be9-be5a-14c549442bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1585 entries, 0 to 1584\n",
      "Data columns (total 55 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   date_of_interest                 1585 non-null   object\n",
      " 1   CASE_COUNT                       1585 non-null   object\n",
      " 2   PROBABLE_CASE_COUNT              1585 non-null   object\n",
      " 3   HOSPITALIZED_COUNT               1585 non-null   object\n",
      " 4   DEATH_COUNT                      1585 non-null   object\n",
      " 5   CASE_COUNT_7DAY_AVG              1585 non-null   object\n",
      " 6   ALL_CASE_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 7   HOSP_COUNT_7DAY_AVG              1585 non-null   object\n",
      " 8   DEATH_COUNT_7DAY_AVG             1585 non-null   object\n",
      " 9   BX_CASE_COUNT                    1585 non-null   object\n",
      " 10  BX_PROBABLE_CASE_COUNT           1585 non-null   object\n",
      " 11  BX_HOSPITALIZED_COUNT            1585 non-null   object\n",
      " 12  BX_DEATH_COUNT                   1585 non-null   object\n",
      " 13  BX_CASE_COUNT_7DAY_AVG           1585 non-null   object\n",
      " 14  BX_PROBABLE_CASE_COUNT_7DAY_AVG  1585 non-null   object\n",
      " 15  BX_ALL_CASE_COUNT_7DAY_AVG       1585 non-null   object\n",
      " 16  BX_HOSPITALIZED_COUNT_7DAY_AVG   1585 non-null   object\n",
      " 17  BX_DEATH_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 18  BK_CASE_COUNT                    1585 non-null   object\n",
      " 19  BK_PROBABLE_CASE_COUNT           1585 non-null   object\n",
      " 20  BK_HOSPITALIZED_COUNT            1585 non-null   object\n",
      " 21  BK_DEATH_COUNT                   1585 non-null   object\n",
      " 22  BK_CASE_COUNT_7DAY_AVG           1585 non-null   object\n",
      " 23  BK_PROBABLE_CASE_COUNT_7DAY_AVG  1585 non-null   object\n",
      " 24  BK_ALL_CASE_COUNT_7DAY_AVG       1585 non-null   object\n",
      " 25  BK_HOSPITALIZED_COUNT_7DAY_AVG   1585 non-null   object\n",
      " 26  BK_DEATH_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 27  MN_CASE_COUNT                    1585 non-null   object\n",
      " 28  MN_PROBABLE_CASE_COUNT           1585 non-null   object\n",
      " 29  MN_HOSPITALIZED_COUNT            1585 non-null   object\n",
      " 30  MN_DEATH_COUNT                   1585 non-null   object\n",
      " 31  MN_CASE_COUNT_7DAY_AVG           1585 non-null   object\n",
      " 32  MN_PROBABLE_CASE_COUNT_7DAY_AVG  1585 non-null   object\n",
      " 33  MN_ALL_CASE_COUNT_7DAY_AVG       1585 non-null   object\n",
      " 34  MN_HOSPITALIZED_COUNT_7DAY_AVG   1585 non-null   object\n",
      " 35  MN_DEATH_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 36  QN_CASE_COUNT                    1585 non-null   object\n",
      " 37  QN_PROBABLE_CASE_COUNT           1585 non-null   object\n",
      " 38  QN_HOSPITALIZED_COUNT            1585 non-null   object\n",
      " 39  QN_DEATH_COUNT                   1585 non-null   object\n",
      " 40  QN_CASE_COUNT_7DAY_AVG           1585 non-null   object\n",
      " 41  QN_PROBABLE_CASE_COUNT_7DAY_AVG  1585 non-null   object\n",
      " 42  QN_ALL_CASE_COUNT_7DAY_AVG       1585 non-null   object\n",
      " 43  QN_HOSPITALIZED_COUNT_7DAY_AVG   1585 non-null   object\n",
      " 44  QN_DEATH_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 45  SI_CASE_COUNT                    1585 non-null   object\n",
      " 46  SI_PROBABLE_CASE_COUNT           1585 non-null   object\n",
      " 47  SI_HOSPITALIZED_COUNT            1585 non-null   object\n",
      " 48  SI_DEATH_COUNT                   1585 non-null   object\n",
      " 49  SI_PROBABLE_CASE_COUNT_7DAY_AVG  1585 non-null   object\n",
      " 50  SI_CASE_COUNT_7DAY_AVG           1585 non-null   object\n",
      " 51  SI_ALL_CASE_COUNT_7DAY_AVG       1585 non-null   object\n",
      " 52  SI_HOSPITALIZED_COUNT_7DAY_AVG   1585 non-null   object\n",
      " 53  SI_DEATH_COUNT_7DAY_AVG          1585 non-null   object\n",
      " 54  INCOMPLETE                       1585 non-null   object\n",
      "dtypes: object(55)\n",
      "memory usage: 681.2+ KB\n"
     ]
    }
   ],
   "source": [
    "def log_message(message):\n",
    "    \"\"\"Function to log general messages\"\"\"\n",
    "    with open('process_log.txt', 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def log_error(error_message):\n",
    "    \"\"\"Function to log errors\"\"\"\n",
    "    with open('error_log.txt', 'a') as f:\n",
    "        f.write(error_message + '\\n')\n",
    "\n",
    "try:\n",
    "    log_message(\"Attempting to establish connection to Azure SQL Database...\")\n",
    "    # Establish a connection to the Azure SQL Database\n",
    "    connection = pyodbc.connect(conn_str)\n",
    "    log_message(\"Connection established successfully!\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    log_message(\"Reading CSV file into pandas DataFrame...\")\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    local_file_path = r\"D:\\Downloads 2\\George Brown\\Sem 2.5\\Big data 2\\Big Data project\\Covid 19\\COVID-19_Daily_Counts_of_Cases__Hospitalizations__and_Deaths.csv\"\n",
    "    df = pd.read_csv(local_file_path)\n",
    "    log_message(\"CSV file read successfully!\")\n",
    "\n",
    "    # Convert data types if necessary\n",
    "    df = df.astype(str)\n",
    "    log_message(\"Converted DataFrame columns to strings.\")\n",
    "\n",
    "    # Print the DataFrame structure\n",
    "    log_message(f\"DataFrame structure: \\n{df.info()}\")\n",
    "\n",
    "    # Use the entire DataFrame for insertion\n",
    "    df_subset = df\n",
    "    log_message(f\"Using all rows of the DataFrame.\")\n",
    "\n",
    "    # Print the first few rows\n",
    "    log_message(f\"First few rows of the DataFrame subset:\\n{df_subset.head()}\")\n",
    "\n",
    "    # Insert DataFrame records one by one, commit in batches\n",
    "    batch_size = 1000  # Adjust the batch size as needed\n",
    "    batch_values = []\n",
    "\n",
    "    for index, row in df_subset.iterrows():\n",
    "        values = tuple(row)\n",
    "        batch_values.append(values)\n",
    "\n",
    "        if (index + 1) % batch_size == 0 or (index + 1) == len(df_subset):\n",
    "            try:\n",
    "                insert_sql = f\"INSERT INTO Covid19_dataNYPD VALUES ({', '.join('?' * len(values))})\"\n",
    "                cursor.executemany(insert_sql, batch_values)\n",
    "                connection.commit()\n",
    "                log_message(f\"Committed {index + 1} rows successfully.\")\n",
    "                batch_values.clear()\n",
    "            except pyodbc.Error as e:\n",
    "                error_message = f\"Error inserting batch ending at row {index + 1}: {str(e)}\"\n",
    "                log_error(error_message)\n",
    "                log_message(error_message)\n",
    "                connection.rollback()  # Rollback the batch on error\n",
    "\n",
    "    # Final commit for any remaining rows\n",
    "    if batch_values:\n",
    "        try:\n",
    "            cursor.executemany(insert_sql, batch_values)\n",
    "            connection.commit()\n",
    "            log_message(f\"Committed final batch successfully.\")\n",
    "        except pyodbc.Error as e:\n",
    "            error_message = f\"Error inserting final batch: {str(e)}\"\n",
    "            log_error(error_message)\n",
    "            log_message(error_message)\n",
    "            connection.rollback()  # Rollback the final batch on error\n",
    "\n",
    "    log_message(\"Data inserted successfully into the 'call_data_lapd' table.\")\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    log_message(\"Connection closed successfully.\")\n",
    "\n",
    "except pyodbc.Error as ex:\n",
    "    # Log the connection error to the file\n",
    "    error_message = f\"Error connecting to Azure SQL Database: {str(ex)}\"\n",
    "    log_error(error_message)\n",
    "    log_message(error_message)\n",
    "\n",
    "except Exception as ex:\n",
    "    # Log any other exceptions\n",
    "    error_message = f\"General error: {str(ex)}\"\n",
    "    log_error(error_message)\n",
    "    log_message(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d264df9-5b8c-4a14-9e89-259b39f98a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
